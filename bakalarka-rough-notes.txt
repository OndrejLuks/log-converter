Zadání
	- Zpracování syrových dat z CAN trace (data logger) s pomocí DBC souborů
	- umožnění exportu do XLS v daném časovém rozmezí
	- GUI nastavení i zobrazení zpracovaných dat
	- import do SQL databáze, definice účtů
	
	
SOURCE INFO
	- Začátky: Lukáš Horký
		- Lukáš Horký dá detailní info (Tomáš pošle kontakt, už je mimo formu)
		- počáteční skript, pár let zpátky
		- skript bral data v MF4 + DBC file
	
	- Další iterace: Milan
		- zeptat se Milana: co, jak a proč upravil
		- stávající stav
		
	- Stávající proces sběru dat
		- každých 50 motohodin stroje se data stahují (1 až 2 krát týdně)
		- z DataLoggeru se vezne SD karta
		- SD do PC -> zkopírují se data do složky a zazipují se na virtuálce
		- Další člověk vezme data, rozzipuje, nahraje do SourceMF4
		- spuštění skriptu na překlad DBC do EXCEL
		- spuštění skruptu na nahrávání do databáze
		- Grafana (staticky nadefinovaná kdysi) si tahá data z databáze z virtuálky
		
		
	- Jaké programy se využívají?
		- nic se nepoužívá, jen tyto skripty
		- Grafana (manuální konfigurace)
		- Dewesoft se používá spíše na měření testů
			- neumí importovat data, hlavně překládá z DBC
			- čistě měřící software
			- není pro analýzu
			- je možné dopřeložit zpětně CAN stream
		
	
	- Kolik času momentálně proces zabere?
		- nejhorší je, že je potřeba více lidí
		- jeden člověk to nahraje, druhý to spustí a třetí to potřebuje
		- blbé je, že ten, co to spouští, vždycky nemá čas.. Často tedy data stojí
		- "lidi mají jiné starosti" - dělat věci navíc není komfortní
		
		- 5 minut z kanclu na dílnu 
		- 30 minut stáhnutí do PC (na jeden server)
		- Kopírování dat na druhý server (virtuálku) - taky dost času
		- spouštění skriptů 5 minut
		- nadefinování grafany od 1 do 3 hodin
		
		
	- Jaké funkcionality jsou klíčové?
			- univerzální zobrazení, chtějí vidět co nejvíc informací
			- ostatní v "nové info - vizualizace"
			
		- Import jakých souborů?
			- MF4, csv, excel
		
		- Export jakých souborů?
			- měla by to umět sama Grafana
			- csv, případně excel
			
		- Upload do databáze?
			- YES
			- Postgre -> Grafana
			
		- Prohlížení lokálně?
			- prohlížení skrze Grafanu na virtuálce
			- grafana a bubble graf? Jde to? Jak to udělat?
		
	
NOVÉ INFO
	- bakalářka bude proces od ničeho do mého skriptu - vyzualizace od grafany s agregací
	- v grafaně musí být volitelné signály (ze všech signálů si člověk vybere, co chce zobrazit)
	- pořád to ale bude proces od MF4+dbc do Grafany
	
	- VIZUALIZACE
		- v grafaně bude 10 grafů, které všechny budou umět porovnávat všechny signály
		- nejlépe více Y os (pro každý zvolený signál)
		- je třeba mít ve všech grafech čas začínající na tom samém místě
			- např. když je moc velká legenda na straně, graf se smrskne a hned je jiný než ten pod tím
		- bylo by dobré vidět motohodiny - kdy se co stalo v motohodinách
		  (lidi si nepamatují, kdy v reálném čase se to stalo)
		  - po CANu chodí motohodiny po 6 minutách
		  - ideálně zvlášť graf motogodin (y) v závisosti na reálném čase
		  
		  
		  
LUKÁŠ HORKÝ - odpověď:
Skripty jsem začal vytvářet v době vývoje první generace stroje E10e (cca 2019).
Python jsem použil, protože jsem se ho tou dobou učil a zpracování dat ze strojů mi přišlo jako dobrý "cvičný" projekt.
Také jsem skripty chtěl "zalepit" některé analytické nedostatky DEWEsoftu a PCAN Exploreru (neco uměl jeden neuměl ten druhý a naopak).
Skripty vznikaly živelně a neměl jsem ambici je využívat nad rámec ladění E10e a učení se s Pythonem.
Původní verze vůbec nepoužívala Postgres ani Grafanu, a každý trace se musel při každém načtení celý překládat
a vizualizaci / analýzu (spotřeby, výkony, runtime, teploty,...) jsem dělal v Jupyteru pomocí Pandas a Pyplotu.
Vzhledem k tomu, že skripty používal právě jeden člověk, tak jsem vůbec neřešil UX.
"Proces" zobrazené dat ze stroje spočíval v zkopírování zdrojového souboru (TRC, nebo MF4) na disk,
úprava cest k souboru a postupné spouštění jednotlivých buněk v Jupyter notebooku.
Velmi brzy se ukázalo, že toto je dost nepoužitelné pro delší záznamy. Takže přišel na řadu Postgres a Grafana.
Jádro, které řešilo překlad CAN zpráv zůstalo, jen se k němu vystavěl wrapper, který řešil nahrávání/překlad dat do DB.
Vizualizaci zajišťovala grafana a případné analýzy jsem dělal pomocí SQL dotazů. Proces byl rozdělen na dvě části - ETL a vizualizaci/analýzu.
ETL spočívalo v zkopírování CAN zpráv z dataloggeru (CANEdge2 MF4) do adresáře a ručním spuštění 2 skriptů - první nahrál raw CAN zprávy do databáze
a druhý provedl překlad CAN zpráv na signály. Pak už bylo možné data prohlížet v Grafaně.
Takže v zásadě postup, který ti ukazoval Kuba Mašek. Po ukončení projektu E10e přešly skripty do "sleep" modu,
kdy se tu a tam přidal stroj z CPG (většinou z iniciativy testovačů), ale neměl jsem už ani chuť ani sílu skripty dál rozvíjet.
Byla tam myšlenka na vytvoření portálu pro správu strojů, generování automatických reportů, ale už na to nedošlo.
			
			
	
ČASOVÝ TEST
	- nová data jsem na starém skriptu nemohl pustit, jelikož to padá na omezení řádků pro excelový sheet. Záznamů v signálu je více jak 1048575,
	  a tak to i s hlavičkou dává 2048576, což už je limit. Celkem F, není to ošetřené pro haldu záznamů.